{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional Classificator using Fashion MNIST dataset\n",
    "\n",
    "![alt text](fashion-mnist_long.png)\n",
    "\n",
    "In this project, we will make a convolutional neural network (hereafter CNN) to classify the different fashion images contained on the [Fashion MNIST dataset](https://www.kaggle.com/zalando-research/fashionmnist). This dataset consists of a set of 70000 images of 28 x 28 pixels in greyscale. Such images can be of 10 different labels, corresponding to \n",
    "\n",
    "| Label number | Label |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "Before we proceed, we will go through a brief explanation of what is a CNN.\n",
    "\n",
    "As it is said in this [IBM article](https://www.ibm.com/topics/convolutional-neural-networks), neural networks are a subset of machine learning, and they are at the heart of deep learning algorithms. They are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node connects to another and has an associated weight and threshold, as it is shown below.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"neural_networks-001.png\" style=\"width:80%\">\n",
    "    <figcaption>Source: https://tikz.net/neural_networks/</figcaption>\n",
    "</figure>\n",
    "\n",
    "The elemental brick of each neural network is an *artificial neuron*, which is described through a model called *perceptron*, whose structure is:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"activation_function.ppm\" style=\"width:80%\">\n",
    "    <figcaption>Source: Google Images (2019)</figcaption>\n",
    "</figure>\n",
    "\n",
    "The idea of such a mechanism is to try to model the real behavior of a biological neuron. The biological neuron takes information sent from other neurons through the dendrites, it is processed by the nucleus of the neuron and it is sent away by the axon. To model this biological mechanism, computer scientists have developed the perceptron, which aims to take data from other neurons through its *weights*, process it with a *linear function*, and 'fire' that information based on the output of the *activation function*.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"neuron.png\" style=\"width:80%\">\n",
    "    <figcaption>Source: https://en.wikipedia.org/wiki/Neuron#/media/File:Blausen_0657_MultipolarNeuron.png</figcaption>\n",
    "</figure>\n",
    "\n",
    "While working with image, speech, or audio signal inputs, *convolutional neural networks* are distinguished from other neural networks by their superior performance. They have three main types of layers, which are:\n",
    "- Convolutional layer\n",
    "- Pooling layer\n",
    "- Fully-connected (FC) layer\n",
    "\n",
    "The *convolutional layer* is the first layer of a convolutional network. While convolutional layers can be followed by additional convolutional layers or *pooling layers*, the *fully-connected* layer is the final layer. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.\n",
    "\n",
    "#### Convolutional layer\n",
    "\n",
    "The convolutional layer is the core building block of a CNN, and it is where the majority of computation occurs. It requires a few components, which are:\n",
    "\n",
    "- **Input data**: Since in this projects we are going to work with clothes images, let’s assume that the input will be a grey color image. It is made up of a matrix of pixels, which means that the input will have two dimensions, a height and width. Since the images are in greyscale, they have just one *channel*. For RGB images, the number of channels is 3.\n",
    "- **Filter**: It is the set of feature detectors, composed of various *kernels*, which will move across the *receptive fields* of the image, checking if the feature is present. This process is known as a *convolution*. The kernel is a two-dimensional (2-D) array of weights, which represents part of the image. They can vary in size, which determines the size of the receptive field. The kernel is then applied to an area of the image, and a dot product is calculated between the input pixels and the kernel. This dot product is then fed into an output array. Afterwards, the kernel shifts by a *stride*, repeating the process until the kernel has swept across the entire image. There will be one kernel per input channel of the convolutional layer and one filter per output channel.\n",
    "- **Feature map**: The final output from the series of dot products from the input and the filter is known as a feature map, activation map, or a convolved feature.\n",
    "\n",
    "After each convolution operation, a CNN applies an *activation function* transformation to the feature map, introducing nonlinearity to the model. \n",
    "\n",
    "As we mentioned earlier, another convolution layer can follow the initial convolution layer. When this happens, the structure of the CNN can become hierarchical as the later layers can see the pixels within the receptive fields of prior layers.\n",
    "\n",
    "<a title=\"Vincent Dumoulin, Francesco Visin, MIT &lt;http://opensource.org/licenses/mit-license.php&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Padding_strides.gif\"><img width=\"512\" alt=\"Convolution arithmetic - Padding strides\" src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif\"></a>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"convolution_example.png\" style=\"width:80%\">\n",
    "    <figcaption>Source: https://developer.nvidia.com/discover/convolution</figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Pooling layer\n",
    "\n",
    "Pooling layers, also known as downsampling, conducts dimensionality reduction, reducing the number of parameters in the input. Its purpose is to gradually shrink the representation’s spatial dimension. Similar to the convolutional layer, the pooling operation sweeps a filter across the entire input, but the difference is that this filter does not have any weights. Instead, the kernel applies an aggregation function to the values within the receptive field, populating the output array. There are two main types of pooling:\n",
    "\n",
    "- **Max pooling**: As the filter moves across the input, it selects the pixel with the maximum value to send to the output array. As an aside, this approach tends to be used more often compared to average pooling.\n",
    "- **Average pooling**: As the filter moves across the input, it calculates the average value within the receptive field to send to the output array.\n",
    "\n",
    "<a title=\"Rafay Qayyum - Introduction To Pooling Layers In CNN\" href=\"https://pub.towardsai.net/introduction-to-pooling-layers-in-cnn-dafe61eabe34\"><img width=\"512\" alt=\"Introduction To Pooling Layers In CNN\" src=\"https://miro.medium.com/v2/resize:fit:828/1*fXxDBsJ96FKEtMOa9vNgjA.gif\"></a>\n",
    "\n",
    "While a lot of information is lost in the pooling layer, it also has a number of benefits to the CNN. They help to reduce complexity, improve efficiency, and limit risk of overfitting.\n",
    "\n",
    "As a final comment, after each kernel is applied in each input channel, the output maps are combined to give as a result one feature map. Thus, as said above, the number of filters will determine the number of output channels in a convolutional layer.\n",
    "\n",
    "<a title=\"Irhum Shafkat - Intuitively Understanding Convolutions for Deep Learning\" href=\"https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1#:~:text=Each%20filter%20in%20a%20convolution,a%20processed%20version%20of%20each.\"><img width=\"1024\" alt=\"Filter's output combination\" src=\"https://miro.medium.com/v2/resize:fit:2000/1*CYB2dyR3EhFs1xNLK8ewiA.gif\"></a>\n",
    "\n",
    "#### Fully-connected layer\n",
    "\n",
    "They are the layers that do not use as inputs 2D or 3D arrays of data, performing a convolution, but take 1D arrays of data and apply to them a linear transformation. They are composed of the perceptrons defined above and their objective is to produce the final output of the entire network.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"fully_cnn.jpg\" style=\"width:100%\">\n",
    "    <figcaption>Source: https://developersbreach.com/convolution-neural-network-deep-learning/</figcaption>\n",
    "</figure>\n",
    "\n",
    "We will implement a CNN whose aim is to predict the labels of the clothes given input images. Let us start by importing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as met\n",
    "\n",
    "# ------------------------------- Plot features ------------------------------\n",
    "# Properties to decorate the plots.\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['font.family'] = 'serif'   \n",
    "plt.rcParams['font.sans-serif'] = 'New Century Schoolbook' # 'Times', 'Liberation Serif', 'Times New Roman'\n",
    "#plt.rcParams['font.serif'] = ['Helvetica']\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['legend.edgecolor'] = 'k'\n",
    "plt.rcParams['legend.markerscale'] = 7\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.major.width']= 0.5\n",
    "plt.rcParams['xtick.major.size']= 5.0\n",
    "plt.rcParams['xtick.minor.width']= 0.5\n",
    "plt.rcParams['xtick.minor.size']= 3.0\n",
    "plt.rcParams['ytick.major.width']= 0.5\n",
    "plt.rcParams['ytick.major.size']= 5.0\n",
    "plt.rcParams['ytick.minor.width']= 0.5\n",
    "plt.rcParams['ytick.minor.size']= 3.0\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we can load the set of images, which is provided by Torch. But before, let us define a custome image dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a custom Dataset class to work the images\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    # We redefine the __len__() method\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    # We redefine the __getitem__() method\n",
    "    def __getitem__(self, i):\n",
    "        image, label = self.dataset[i]\n",
    "        # We rewrite the original label to allow it be able to be compared against the model predictions\n",
    "        zer = torch.zeros(10)\n",
    "        zer[label] = 1.\n",
    "        label = zer\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download training data\n",
    "train_d = FashionMNIST(\n",
    "    root='Dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_data = train_d\n",
    "train_data = CustomImageDataset(train_data)\n",
    "\n",
    "# Download test data\n",
    "test_d = FashionMNIST(\n",
    "    root='Dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = test_d\n",
    "test_data = CustomImageDataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have loaded the Dataset objects it is time to instantiate the Dataloader objects in order to get the proper inputs to the CNN. Also, it is possible to train the CNN in batches if the inputs are Dataloader objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the batch of images\n",
    "batch_size = 1000\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceed, let us talk a little bit about **Torch tensors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor definition\n",
    "t = torch.tensor([1.0, 3.5, -np.pi, 0.0, 9.0], dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t), \"\\n\", t.size(), \"\\n\", t.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t*t, \"\\n\", t+t, \"\\n\", t[-1], \"\\n\", t.view(1, 5), \"\\n\", t.view(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.reshape(1, 5), \"\\n\", t.reshape(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some mathematical functions\n",
    "print(torch.sin(t), \"\\n\", torch.arctan(t), \"\\n\", t.cosh(), \"\\n\", t.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(1.0, 6.0, 1.0)\n",
    "print(a, \"\\n\")\n",
    "print(torch.pow(t, 2.0), \"\\n\", t.pow(2), \"\\n\", t.pow(a), \"\\n\")\n",
    "a = torch.linspace(1.0, 6.0, 5)\n",
    "print(a, \"\\n\")\n",
    "print(torch.pow(t, 2.0), \"\\n\", t.pow(2), \"\\n\", t.pow(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t == t, \"\\n\", torch.ge(t, 2.0*t), \"\\n\", t.isnan(), \"\\n\", t.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some statistical functions\n",
    "print(t.mean(), \"\\n\", t.mode(), \"\\n\", t.median(), \"\\n\", t.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran = torch.rand(2, 2, 3)\n",
    "print(ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_int = torch.randint(0, 100, (2, 2, 3))\n",
    "print(ran_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to the main work, we can check if cuda is available for training. The use of cuda optimizes the training process, allowing us to use the different GPUs we have in our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have loaded the dataset and we transformed it in appropiate Dataloader objects, it is time to define the model we will train to predict the label of the image used as input. As said before, the model is a CNN, and the architecture of such a network will be explained below. Before, we need to define the hyperparameters of the CNN, as usual in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_inputs = 196\n",
    "n_hidden = 98\n",
    "n_outputs = 10\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "kernel_size_1 = 7\n",
    "kernel_size_2 = 5\n",
    "p_dropout = 0.1   # Dropout probability\n",
    "lr = 1e-3   # Learning rate\n",
    "n_epochs = 300   # Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Model(nn.Module):\n",
    "    # Define model elements\n",
    "    def __init__(self):\n",
    "        \"\"\" The super() builtin returns a proxy object (temporary object of the superclass) which\n",
    "        let's you avoid referring to the base class explicitly and it allows us to access methods \n",
    "        of the base class. \"\"\"\n",
    "        super().__init__()\n",
    "        # Sequence of transformations implemented by the layers of the network\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size_2, stride=1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_inputs, n_hidden),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_outputs),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    # Method to transform inputs in outputs considering the internal structure of the network\n",
    "    def forward(self, X):\n",
    "        output = self.cnn(X)\n",
    "        return output\n",
    "    \n",
    "# Now we can create a model and send it at once to the device\n",
    "model = Model().to(device)\n",
    "# We can also inspect its parameters using its state_dict() method\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](red.JPG)\n",
    "###### Architecture of the CNN\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    Softmax(x_{i}) = \\frac{\\mathrm{e}^{x_{i}}}{\\Sigma_{j=1}^{N}\\mathrm{e}^{x_{j}}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<figure>\n",
    "    <img src=\"sigmoid.png\" style=\"width:80%\">\n",
    "    <figcaption>Source: https://www.researchgate.net/figure/A-Basic-sigmoid-function-with-two-parameters-c1-and-c2-as-commonly-used-for-subitizing_fig2_325868989</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined the model, we want to train it. So, let us take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels of the clothes based on the table given at the beggining of the notebook\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "# Plot of a random sample of tain_data\n",
    "figure = plt.figure(figsize=(5, 5), dpi=180)\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols*rows + 1):\n",
    "    sample_idx = torch.randint(len(train_d), size=(1,)).item()\n",
    "    img, label = train_d[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('examples.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make some statistical analysis regarding the labels. In doing so, we are going to be able to see if the dataset is imbalanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us convert the targets tensors into a pandas DataFrame object to make easier the computations\n",
    "df_train = pd.DataFrame(train_d.targets.numpy(), columns = ['Labels'])\n",
    "df_test = pd.DataFrame(test_d.targets.numpy(), columns = ['Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a DataFrame object we can inspect the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training labels are perfectly balanced, making the training process easier than in the case of imbalanced classes. Note that we have labels that are encoded, so it is not needed to apply the label encoding method on the targets. To move forward, we have to define the function that will perform the training of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the training function\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = int(len(dataloader.dataset)/1000)\n",
    "    tmp = []\n",
    "\n",
    "    # We iterate over batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # We calculate the model's prediction\n",
    "        pred = model(X)\n",
    "        # With the model's prediction we calculate the loss function\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # We apply the backpropagation method\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Training progress\n",
    "        loss, current = loss.item(), batch\n",
    "        tmp.append(loss)\n",
    "        print(f\"Actual batch = {current} | Loss = {loss:>7f} | Processed samples: [{current:>2d}/{size:>2d}]\")\n",
    "    \n",
    "    tmp = np.array(tmp)\n",
    "    loss_avg = tmp.sum()/len(tmp)\n",
    "    return loss_avg\n",
    "\n",
    "# We define the test function\n",
    "def test_loop(dataloader, model, loss_fn, num_batches):\n",
    "    size = int(len(dataloader.dataset)/1000)\n",
    "    test_loss = 0\n",
    "    j = 0\n",
    "    \n",
    "    # To test, we need to deactivate the calculation of the gradients\n",
    "    with torch.no_grad():\n",
    "        # We iterate over batches\n",
    "        for X, y in dataloader:\n",
    "            # Model's prection\n",
    "            pred = model(X)\n",
    "            # Corresponding errors, which we acumulate in a total value\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            j += 1\n",
    "            \n",
    "    # We calculate the total loss and print it\n",
    "    test_loss /= j\n",
    "    print(f\"Test Error: Avg loss = {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model, we need to instanciate an optimizer object and a loss function object. Let us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function object. It is a Medium Squared Error.\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# We instantiate an optimizer. In this case we choose an Adam optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict size to gain some perspective about the model\n",
    "print(\"Model's state_dict size:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the loss function against the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We define a loss array to plot the training loss function and the testing loss function.\n",
    "Comment if you have to load a trained model.\n",
    "loss_to_plot = []\n",
    "loss_to_plot_test = []\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train the model. Let us train it during $n_{epochs}$ epochs, as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We train the model iterating over the different epochs. Comment if you have to load a trained model.\n",
    "for t in range(n_epochs):\n",
    "    print(f\"Epoch {t+1}\\n=============================================\")\n",
    "    loss_to_plot.append(train_loop(train_dl, model, loss_fn, optimizer))\n",
    "    loss_to_plot_test.append(test_loop(test_dl, model, loss_fn, batch_size))\n",
    "print(\"Done!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained. It is better to save it before continue doing other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in .pth file. Comment if you have to load a trained model.\n",
    "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "\n",
    "# To load it just uncomment the following lines\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to check the precision of the predictions. In order to do so, we will plot both loss functions and we will make a *confusion matrix* plot to see the tendencies of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save both loss functions. Comment if you have to load a trained model.\n",
    "#np.savetxt('loss_to_plot.txt', loss_to_plot)\n",
    "#np.savetxt('loss_to_plot_test.txt', loss_to_plot_test)\n",
    "\n",
    "# We choose an image and calculate the corresponding prediction generated by the model\n",
    "ind = 78\n",
    "for (X, y) in test_dl:\n",
    "    pred_cpu = model(X)\n",
    "    image_cpu = X[ind]\n",
    "    break\n",
    "\n",
    "pred_cpu = pred_cpu[ind].detach().numpy()\n",
    "\n",
    "# We plot the image to be predicted and as a title the corresponding prediction\n",
    "fig, ax = plt.subplots(1, 1, dpi=180)\n",
    "fig.set_size_inches(4.0, 4.0)\n",
    "ax.axis(\"off\")\n",
    "plt.title(labels_map[np.argmax(pred_cpu)])\n",
    "ax.imshow(image_cpu.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = np.loadtxt('loss_to_plot.txt')\n",
    "lp_test = np.loadtxt('loss_to_plot_test.txt')\n",
    "\n",
    "# Let us plot both loss functions\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 6), dpi=200)\n",
    "ax.plot([i for i in range(3, n_epochs+1)], lp[2:], color='darkblue', lw=1.5, label='Training error')\n",
    "ax.plot([i for i in range(3, n_epochs+1)], lp_test[2:], ls=':', color='maroon', lw=1.5, label='Test error')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Average loss function')\n",
    "ax.set_xticks([0, 75, 150, 225, 300])\n",
    "ax.set_xticklabels(['0', '75', '150', '225', '300'])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid the model being overfitted, it is needed to train it during 300 epochs, because if it is trained during a greater number of epochs, it can be seen a very small tendency of the average test loss function to start growing.\n",
    "\n",
    "To conclude the analisys of the precision of the model, let us make a confusion matrix. That way, we will be able to see the tendencies of the model at the time of predict the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a function that get all the predictions made by the CNN\n",
    "@torch.no_grad() # turn off gradients during inference for memory efficiency\n",
    "def get_all_preds(network, dataloader):\n",
    "    \"\"\" Function to return the number of correct predictions across data set \"\"\"\n",
    "    all_preds = torch.tensor([])\n",
    "    model = network\n",
    "    tmp_labels = np.array([])\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        tmp_labels = np.concatenate((tmp_labels, labels.argmax(1).numpy()))\n",
    "        preds = model(images) # get preds\n",
    "        all_preds = torch.cat((all_preds, preds), dim=0) # join along existing axis\n",
    "    \n",
    "    return all_preds, tmp_labels\n",
    "\n",
    "# Let us define the function that plots the confusion matrix\n",
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm)/np.sum(cm).astype('float')\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')    # Choose Blues by default\n",
    "\n",
    "    plt.figure(figsize=(12, 10), dpi=300)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "\n",
    "    thresh = cm.max()/1.5 if normalize else cm.max()/2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.ylabel('Correct class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once defined the functions, let us get all the predictions made over the test set and plot them in the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions over the test set\n",
    "all_preds_test, labels_test = get_all_preds(network=model, dataloader=test_dl)\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm = met.confusion_matrix(y_true=labels_test, y_pred=all_preds_test.argmax(1).numpy())\n",
    "\n",
    "# Plot the predictions as a confusion matrix\n",
    "plot_confusion_matrix(cm, target_names=labels_map.values(), cmap='Blues', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] [A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n",
    "\n",
    "[2] [Fashion MNIST classification using custom PyTorch Convolution Neural Network (CNN)](https://boscoj2008.github.io/customCNN/)\n",
    "\n",
    "[3] [Convolutional Neural Networks](https://www.ibm.com/cloud/learn/convolutional-neural-networks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
